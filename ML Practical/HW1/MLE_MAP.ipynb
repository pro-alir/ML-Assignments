{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1-izC4ZKHJ9"
      },
      "source": [
        "<h1 style=\"text-align: center\">\n",
        "Machine Learning </br>\n",
        "MLE & MAP in Python\n",
        "</h1>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhTuYwHYbE_1"
      },
      "source": [
        "## Objective\n",
        "This exercise will help you gain a deeper understanding of, and insights into, Maximum Likelihood Estimation (MLE) and Maximum A Posteriori (MAP) estimation$\\textit{Maximum Likelihood Estimation (MLE) and Maximum A Posteriori (MAP) }$ :) \\\\\n",
        "Let’s say you have a barrel of apples that are all different sizes. You pick an apple at random, and you want to know its weight. Unfortunately, all you have is a broken scale. answer the questions below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSaLb6AYa9DJ"
      },
      "source": [
        "1) For the sake of this section, lets imagine a farmer tells you that the scale returns the weight of the object with an error of +/- a standard deviation of 5g. We can describe this mathematically as:\n",
        "$$\n",
        "measurement = weight + \\mathcal{N}(0, 5g)\n",
        "$$\n",
        "You can weigh the apple as many times as you want, so weigh it 100 times.\n",
        "plot its histogram of your 100 measurements. (y axis is the counts and x-axis is the measured weight)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "hPMnHTcia07a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "outputId": "4c90f604-8f19-4633-84ee-ccd50747ef9d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# I assumed the true weight is 150\n",
        "true_weight = 150\n",
        "std_dev = 5\n",
        "measurements = np.random.normal(true_weight, std_dev, 100)\n",
        "\n",
        "plt.hist(measurements, bins=20, edgecolor='black')\n",
        "plt.ylabel('Counts')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HD9Mqy-bcPi5"
      },
      "source": [
        "2) Find the average weight of the apple.\n",
        "Is it a good guess? state your reason."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "xlCBTC0lcPKa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e15897c3-6337-4490-9e97-850776651c36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The average weight is:  150.2486271513936\n"
          ]
        }
      ],
      "source": [
        "average_weight = measurements.mean()\n",
        "print('The average weight is: ', average_weight)\n",
        "\n",
        "# This is a good guess. This is because of central limit theorem.\n",
        "# Base on central limit theorem, the mean of a large number of independent and identically distributed variables will be normally distributed around the true mean, as long as the number of variables is large enough."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-P9PJuKcrbq"
      },
      "source": [
        "3) we are going to use grid approximation for calculating the MLE. here is the link if you wnat to get more fimilar with this technique:\n",
        "https://www.bayesrulesbook.com/chapter-6\n",
        "\n",
        "Our end goal is to find the weight of the apple, given the data we have. To formulate it in a Bayesian way: We’ll ask what is the probability of the apple having weight, $w$, given the measurements we took, $X$. And, because we're formulating this in a Bayesian way, we use Bayes’ Law to find the answer:\n",
        "\n",
        "$$\n",
        "P(w|X) = \\frac{P(X|w)P(w)}{P(X)}\n",
        "$$\n",
        "\n",
        "If we make no assumptions about the initial weight of our apple, then we can drop $P(w)$. We’ll say all sizes of apples are equally likely (we’ll revisit this assumption in the MAP approximation).\n",
        "\n",
        "Furthermore, we’ll drop $P(X)$ - the probability of seeing our data. This is a normalization constant and will be important if we do want to know the probabilities of apple weights. But, for right now, our end goal is to only to find the most probable weight. $P(X)$ is independent of $w$, so we can drop it if we’re doing relative comparisons.\n",
        "\n",
        "This leaves us with $P(X|w)$, our likelihood, as in, what is the likelihood that we would see the data, $X$, given an apple of weight $w$. If we maximize this, we maximize the probability that we will guess the right weight.\n",
        "\n",
        "The grid approximation is probably the simplest way to do this. Basically, we’ll systematically step through different weight guesses, and compare what it would look like if this hypothetical weight were to generate data. We’ll compare this hypothetical data to our real data and pick the one that matches the best.\n",
        "\n",
        "To formulate this mathematically:\n",
        "\n",
        "For each of these guesses, we’re asking \"what is the probability that the data we have, came from the distribution that our weight guess would generate\". Because each measurement is independent from another, we can break the above equation down into finding the probability on a per measurement basis:\n",
        "\n",
        "$$\n",
        "P(X|w) = \\prod_{i}^{N} p(x_i|w)\n",
        "$$\n",
        "\n",
        "So, if we multiply the probability that we would see each individual data point - given our weight guess - then we can find one number comparing our weight guess to all of our data.\n",
        "\n",
        "The peak in the likelihood is the weight of the apple.\n",
        "\n",
        "To make it computationally easier,\n",
        "\n",
        "$$\n",
        "\\log P(X|w) = \\log \\prod_{i}^{N} p(x_i|w) = \\sum_{i}^{N} \\log p(d_i|w)\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "a) Why did we use log likelihood? Is it ok to do so?\n",
        "\n",
        "**Answer**: Yes, it is ok to use log likelihood. We use this because it turns products to sums, and this is easier to compute and easier to work with, especially wen probabilities are too small. This is okay because log is strictly monotonic and preserves the order of the values.\n",
        "\n",
        "\n",
        "b) do the grid approximation and complete the cell below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "9NnWmxzTiRfr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "093841ef-ef99-499b-8ed3-fd89bcb4a4a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLE: 151.0204081632653\n"
          ]
        }
      ],
      "source": [
        "from scipy.stats import norm\n",
        "import numpy as np\n",
        "\n",
        "# since I assumed the true weight is 150, I changed this line of code to a reasonable interval\n",
        "weight_grid = np.linspace(100, 200)\n",
        "# print(weight_grid)\n",
        "log_likelihoods = []\n",
        "\n",
        "for weight_guess in weight_grid:\n",
        "    log_likelihood = np.sum(norm.logpdf(measurements, loc=weight_guess, scale=5))\n",
        "    log_likelihoods.append(log_likelihood)\n",
        "\n",
        "log_likelihoods = np.array(log_likelihoods)\n",
        "\n",
        "mle_weight = weight_grid[np.argmax(log_likelihoods)]\n",
        "print(\"MLE:\", mle_weight)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NN3lt2npcc2S"
      },
      "source": [
        "Play around with the code and try to answer the following questions regarding MLE and MAP. You can draw plots to visualize as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "ezcWTpNQamCL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b348445-5afd-4478-d8b8-317fe039dac7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average measurement: 93.261 g\n",
            "Maximum Likelihood estimate: 93.465 g\n",
            "Maximum A Posterior estimate: 51.253 g\n",
            "The true weight of the apple was: 93.977 g\n"
          ]
        }
      ],
      "source": [
        "# After changinge the numbers of error, number of measurements, and prior, I set them to their default values.\n",
        "\n",
        "import numpy as np\n",
        "from scipy.stats import norm, invgamma\n",
        "\n",
        "\n",
        "# The barrel of apples\n",
        "# The average apples is between 70-100 g\n",
        "BARREL = np.random.normal(loc=85, scale=20, size=100)\n",
        "# Grid\n",
        "WEIGHT_GUESSES = np.linspace(1, 200, 100)\n",
        "ERROR_GUESSES = np.linspace(.1, 50, 100)\n",
        "\n",
        "# NOTE: Try changing the scale error\n",
        "# in practice, you would not know this number\n",
        "SCALE_ERR = 5\n",
        "\n",
        "# NOTE: Try changing the number of measurements taken\n",
        "N_MEASURMENTS = 10\n",
        "\n",
        "# NOTE: Try changing the prior values and distributions\n",
        "PRIOR_WEIGHT = norm(50, 1).logpdf(WEIGHT_GUESSES)\n",
        "PRIOR_ERR = invgamma(4).logpdf(ERROR_GUESSES)\n",
        "\n",
        "LOG_PRIOR_GRID = np.add.outer(PRIOR_ERR, PRIOR_WEIGHT)\n",
        "\n",
        "\n",
        "def read_scale(apple):\n",
        "    return apple + np.random.normal(loc=0, scale=SCALE_ERR)\n",
        "\n",
        "\n",
        "def get_log_likelihood_grid(measurments):\n",
        "    log_liklelihood = [\n",
        "        [\n",
        "            norm(weight_guess, error_guess).logpdf(measurments).sum()\n",
        "            for weight_guess in WEIGHT_GUESSES\n",
        "        ]\n",
        "        for error_guess in ERROR_GUESSES\n",
        "    ]\n",
        "    return np.asarray(log_liklelihood)\n",
        "\n",
        "\n",
        "def get_mle(measurments):\n",
        "    log_likelihood = get_log_likelihood_grid(measurments)\n",
        "    max_likelihood_index = np.argwhere(log_likelihood == log_likelihood.max())[0][1]\n",
        "    return WEIGHT_GUESSES[max_likelihood_index]\n",
        "\n",
        "\n",
        "\n",
        "def get_map(measurements):\n",
        "    log_posterior = get_log_likelihood_grid(measurements) + LOG_PRIOR_GRID\n",
        "    max_posterior_index = np.argwhere(log_posterior == log_posterior.max())[0][1]\n",
        "    return WEIGHT_GUESSES[max_posterior_index]\n",
        "\n",
        "# Pick an apple at random\n",
        "apple = np.random.choice(BARREL)\n",
        "\n",
        "# weight the apple\n",
        "measurments = np.asarray([read_scale(apple) for _ in range(N_MEASURMENTS)])\n",
        "\n",
        "print(f\"Average measurement: {measurments.mean():.3f} g\")\n",
        "print(f\"Maximum Likelihood estimate: {get_mle(measurments):.3f} g\")\n",
        "print(f\"Maximum A Posterior estimate: {get_map(measurments):.3f} g\")\n",
        "print(f\"The true weight of the apple was: {apple:.3f} g\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LI_541TpetKk"
      },
      "source": [
        "<h3><i><i> Questions</h3>\n",
        "1.\n",
        "How sensitive is the MAP measurement to the choice of prior?\n",
        "\n",
        "**Answer:**\n",
        "The MAP estimate is sensitive to the choice of prior. This is because MAP puts prior into the estimation process. If the prior is very informative or biased, it can highly impact on the MAP estimate. A strong prior can overcome small datasets,  influencing the MAP estimate on the prior's peak. The shape and spread of the prior distribution is really important in MAP. low variance prior makes the MAP estimate highly sensitive to where this prior is centered. high variance prior makes map  more reliant on the data. If prior and data have a lot of differences, the MAP estimate can be heavily biased towards the prior belief."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMV-wgYXes_O"
      },
      "source": [
        "<h3><i><i></h3>\n",
        "2. How sensitive is the MLE and MAP answer to the grid size?\n",
        "\n",
        "**Answer:**\n",
        "The grid size affects the resolution of the parameter estimation. A grid with more points and smaller intervals between points causes more precise approximation for MLE or MAP. However, after a certain point, the accuracu doesn't improve significantly. But inctreasing the grid size causes more computational costs. With a big grid, there's a risk of missing a certain point where MLE or MAP can give a better estimation."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}